{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":38613,"sourceType":"datasetVersion","datasetId":30292}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Loading the dataset to understand structure and contents\n\ndf = pd.read_csv(\"/kaggle/input/avocado-prices/avocado.csv\")\ndf.head()\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T13:05:49.494621Z","iopub.execute_input":"2024-04-29T13:05:49.495054Z","iopub.status.idle":"2024-04-29T13:05:51.093427Z","shell.execute_reply.started":"2024-04-29T13:05:49.495009Z","shell.execute_reply":"2024-04-29T13:05:51.091845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data preprocessing\n\n# Convert date to datetime format\n\ndf['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\ndf.sort_values('Date', inplace=True)\nmissing_values = df.isnull().sum()\n\n#Plotting the average price of avocados over time\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,6))\nplt.plot(df['Date'], df['AveragePrice'], label='Average Price')\nplt.title('Average Price of Avocados Over Time')\nplt.xlabel('Date')\nplt.ylabel('Average Price ($)')\nplt.legend()\nplt.show()\n\nmissing_values","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:06:06.119366Z","iopub.execute_input":"2024-04-29T13:06:06.119900Z","iopub.status.idle":"2024-04-29T13:06:06.810811Z","shell.execute_reply.started":"2024-04-29T13:06:06.119860Z","shell.execute_reply":"2024-04-29T13:06:06.809841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Trend and Seasonality**: The plot of average avocado prices over time shows both trends and possible seasonal fluctuations, indicating that prices vary across different times of the year. This seasonality and trend component will be important to model accurately to predict future prices.\n**No Missing Values**: There are no missing values in the dataset, which simplifies the preprocessing step as we don't need to handle missing data.\n\nGiven these observations, we can proceed with feature engineering to extract more useful features for our model, such as month and year from the date. These features can help capture the seasonal patterns observed in the data. Given the clear presence of seasonality and possibly non-linear trends, models that can capture these elements, such as SARIMA or LSTM, might be particularly well-suited for this task.\n\nLet's proceed by extracting the month and year from the date, and preparing the dataset for modeling. We will focus on using a time series model that can handle seasonality. For this demonstration, let's consider a SARIMA model, which is designed to handle both seasonal and non-seasonal data. We'll start by preparing our features and then fit a SARIMA model to the data.\n","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\naverage_price_per_date = df.groupby('Date')['AveragePrice'].mean().reset_index()\n\naverage_price_per_date.set_index('Date', inplace=True)\n\ndecomposition = seasonal_decompose(average_price_per_date['AveragePrice'], model='additive', period=52)\n\nfig = decomposition.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:06:16.975998Z","iopub.execute_input":"2024-04-29T13:06:16.976425Z","iopub.status.idle":"2024-04-29T13:06:20.166450Z","shell.execute_reply.started":"2024-04-29T13:06:16.976394Z","shell.execute_reply":"2024-04-29T13:06:20.164848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The decomposition of the avocado prices time series into trend, seasonal, and residual components reveals:\n\n1. **Trend**: There's a clear trend component that varies over time, indicating changes in average prices across the years.\n2. **Seasonality**: The seasonal component shows consistent patterns within each year, confirming the presence of seasonality in avocado prices. This seasonality is likely related to factors such as growing seasons and consumer demand patterns.\n3. **Residuals**: The residuals, which represent the noise in the data after removing trend and seasonality, seem to be relatively small but with some spikes, indicating occasional deviations from the pattern explained by trend and seasonality.\nGiven these observations, a model like **SARIMA (Seasonal Autoregressive Integrated Moving Average)**, which is designed to handle both the trend and seasonality in time series data, seems appropriate for forecasting avocado prices.\n\nThe next steps involve:\n\n1. **Model Selection**: Choosing appropriate parameters for the SARIMA model. This typically involves identifying the order of the autoregressive (AR), differencing (I), and moving average (MA) components, as well as the seasonal components of these parameters.\n2. **Model Training**: Fitting the SARIMA model to the historical data.\n3. **Model Evaluation**: Validating the model's performance using a hold-out sample or through cross-validation techniques specific to time series data, such as time series cross-validation.","metadata":{}},{"cell_type":"code","source":"# Assuming average_price_per_date['AveragePrice'] is your series\ndata_series = average_price_per_date['AveragePrice']\n\n# Calculate the index to split the data\nsplit_point = int(len(data_series) * 0.75)\n\n# Split the data into training and testing sets\ntrain = data_series[:split_point]\ntest = data_series[split_point:]\n\n# Display the sizes of the training and testing sets\nprint(f\"Training Set Size: {len(train)}\")\nprint(f\"Testing Set Size: {len(test)}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:06:26.389130Z","iopub.execute_input":"2024-04-29T13:06:26.389861Z","iopub.status.idle":"2024-04-29T13:06:26.397073Z","shell.execute_reply.started":"2024-04-29T13:06:26.389821Z","shell.execute_reply":"2024-04-29T13:06:26.396231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.plot(train.index, train, label='Training Set', color='blue')\nplt.plot(test.index, test, label='Testing Set', color='red')\nplt.title('Training and Testing Data Split')\nplt.xlabel('Date')\nplt.ylabel('Average Price')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:07:32.213924Z","iopub.execute_input":"2024-04-29T13:07:32.214402Z","iopub.status.idle":"2024-04-29T13:07:32.726725Z","shell.execute_reply.started":"2024-04-29T13:07:32.214368Z","shell.execute_reply":"2024-04-29T13:07:32.725320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import zscore\n\n# Calculate Z-scores of the data\ntrain_zscores = zscore(train)\n\n# Define outliers as those where the absolute Z-score is greater than 3\noutliers = train[(abs(train_zscores) > 2)]\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(train.index, train, label='Training Data', color='blue')\nplt.scatter(outliers.index, outliers, color='red', label='Outliers')\nplt.title('Outlier Analysis')\nplt.xlabel('Date')\nplt.ylabel('Average Price')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:07:36.160715Z","iopub.execute_input":"2024-04-29T13:07:36.161167Z","iopub.status.idle":"2024-04-29T13:07:36.671557Z","shell.execute_reply.started":"2024-04-29T13:07:36.161132Z","shell.execute_reply":"2024-04-29T13:07:36.670397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pmdarima\nfrom pmdarima import auto_arima\n\nsarima_model = auto_arima(train, seasonal=True, m=52, trace=True, error_action='ignore', stepwise=True)\n\nsarima_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:07:40.860860Z","iopub.execute_input":"2024-04-29T13:07:40.861286Z","iopub.status.idle":"2024-04-29T13:08:28.715733Z","shell.execute_reply.started":"2024-04-29T13:07:40.861249Z","shell.execute_reply":"2024-04-29T13:08:28.713276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Generating forecasts for the test set\nforecasts = sarima_model.predict(n_periods=len(test))\n\n# Calculate error metrics\nmae = mean_absolute_error(test, forecasts)\nrmse = np.sqrt(mean_squared_error(test, forecasts))\n\nprint(f'MAE: {mae}')\nprint(f'RMSE: {rmse}')\n\n# Plot actual vs forecasted values\nplt.figure(figsize=(10, 6))\nplt.plot(test.index, test, label='Actual', color='blue')\nplt.plot(test.index, forecasts, label='Forecast', color='red')\nplt.title('Actual vs Forecasted Values')\nplt.xlabel('Date')\nplt.ylabel('Average Price')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:09:24.364198Z","iopub.execute_input":"2024-04-29T13:09:24.364864Z","iopub.status.idle":"2024-04-29T13:09:24.840891Z","shell.execute_reply.started":"2024-04-29T13:09:24.364826Z","shell.execute_reply":"2024-04-29T13:09:24.839505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/avocado-prices/avocado.csv\")\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Extract time-related features\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['DayOfWeek'] = df['Date'].dt.dayofweek\n\n# Features and target variable\nfeatures = ['Year', 'Month', 'DayOfWeek', 'Total Volume', '4046', '4225', '4770']\n\n# Split the data into features and target variable\nX = df[features]\ny = df['AveragePrice']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build LightGBM model\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 29,\n    'learning_rate': 0.4,\n    'feature_fraction': 1\n}\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_model = lgb.train(params, lgb_train, num_boost_round=100)\n\n# Make predictions on the test set\ntest_predictions = lgb_model.predict(X_test)\n\n# Create a DataFrame to store the actual and predicted values with corresponding dates\npredictions_df = pd.DataFrame({'Date': X_test.index, 'Actual': y_test.values, 'Predicted': test_predictions})\n\n# Sort the DataFrame by the 'Date' column\npredictions_df = predictions_df.sort_values(by='Date')\n\n# Select the last 7 days of data\nlast_week_predictions = predictions_df.iloc[-56:]\n\n# Plot actual vs predicted values for the last 7 days\nplt.figure(figsize=(12, 6))\nplt.plot(last_week_predictions['Date'], last_week_predictions['Actual'], label='Actual', color='blue')\nplt.plot(last_week_predictions['Date'], last_week_predictions['Predicted'], label='Predicted', color='red')\nplt.title(f'Actual vs Predicted Values RMSE:{rmse}')\nplt.xlabel('Date')\nplt.ylabel('Average Price')\nplt.legend()\nplt.show()\n\n# Evaluate the model\nrmse = np.sqrt(mean_squared_error(y_test, test_predictions))\nprint(f'Root Mean Squared Error: {rmse}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:09:30.309712Z","iopub.execute_input":"2024-04-29T13:09:30.310115Z","iopub.status.idle":"2024-04-29T13:09:32.557313Z","shell.execute_reply.started":"2024-04-29T13:09:30.310086Z","shell.execute_reply":"2024-04-29T13:09:32.555636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/avocado-prices/avocado.csv\")\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Extract time-related features\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['DayOfWeek'] = df['Date'].dt.dayofweek\n\n# Calculate average price per date\naverage_price_per_date = df.groupby('Date')['AveragePrice'].mean().reset_index()\n\n# Set the 'Date' column as the index\naverage_price_per_date.set_index('Date', inplace=True)\n\n# Perform seasonal decomposition\ndecomposition = seasonal_decompose(average_price_per_date['AveragePrice'], model='additive', period=52)\n\n# Use the trend component as your target variable (y)\ny = decomposition.trend.dropna()\n\n# Features (you can modify this based on your requirements)\nfeatures = ['Year', 'Month', 'DayOfWeek', 'Total Volume', '4046', '4225', '4770']\n\n# Create features and target variable\nX = df.set_index('Date')[features].loc[y.index]\ny = y.loc[X.index]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build LightGBM model\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,  # Adjusted value\n    'learning_rate': 0.4,  # Adjusted value\n    'feature_fraction': 1,\n    'lambda_l1': 1,  # Regularization parameters\n    'lambda_l2': 1\n}\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_model = lgb.train(params, lgb_train, num_boost_round=100)\n\n# Make predictions on the test set\ntest_predictions = lgb_model.predict(X_test)\n\n# Apply exponential smoothing to predictions using pandas\nalpha = 1\nsmoothed_predictions = pd.Series(test_predictions).ewm(alpha=alpha).mean()\n\n# Sort by date before plotting\nsorted_indices = np.argsort(X_test.index)\nX_test_sorted = X_test.iloc[sorted_indices]\ny_test_sorted = y_test.iloc[sorted_indices]\nsmoothed_predictions_sorted = smoothed_predictions.iloc[sorted_indices]\n\n# Plot actual vs smoothed predicted values for the last 28 days\nlast_28_days = 5000\nplt.figure(figsize=(12, 6))\nplt.plot(X_test_sorted.index[-last_28_days:], y_test_sorted.values[-last_28_days:], label='Actual', color='blue')\nplt.plot(X_test_sorted.index[-last_28_days:], smoothed_predictions_sorted[-last_28_days:], label='Smoothed Predicted', color='red')\nplt.title(f'Actual vs Smoothed Predicted Values, RMSE:{rmse}')\nplt.xlabel('Date')\nplt.ylabel('Average Price (Trend Component)')\nplt.legend()\nplt.show()\n\n# Evaluate the model\nrmse = np.sqrt(mean_squared_error(y_test_sorted, smoothed_predictions_sorted))\nprint(f'Root Mean Squared Error: {rmse}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:11:20.772553Z","iopub.execute_input":"2024-04-29T13:11:20.773188Z","iopub.status.idle":"2024-04-29T13:11:21.487421Z","shell.execute_reply.started":"2024-04-29T13:11:20.773142Z","shell.execute_reply":"2024-04-29T13:11:21.486280Z"},"trusted":true},"execution_count":null,"outputs":[]}]}